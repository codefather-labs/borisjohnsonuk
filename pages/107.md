`def` ` ` `cnn_articles` `(` `url` `,` ` ` `page_data` `):` `  ` `    ` `soup` ` ` `=` ` ` `BeautifulSoup` `(` `page_data` `,` ` ` `'lxml'` `)` `    ` `def` ` ` `match` `(` `tag` `):` `        ` `return` ` ` `(` `            ` `tag` `.` `text` ` ` `and` ` ` `tag` `.` `has_attr` `(` `'href'` `)` `            ` `and` ` ` `tag` `[` `'href'` `]` `.` `startswith` `(` `'/'` `)` `            ` `and` ` ` `tag` `[` `'href'` `]` `.` `endswith` `(` `'.html'` `)` `            ` `and` ` ` `tag` `.` `find` `(` `class_` `=` `'cd__headline-text'` `)` `        ` `)` `    ` `headlines` ` ` `=` ` ` `soup` `.` `find_all` `(` `match` `)` `  ` `    ` `return` ` ` `[(` `url` ` ` `+` ` ` `hl` `[` `'href'` `],` ` ` `hl` `.` `text` `,` ` ` `'cnn'` `)` `            ` `for` ` ` `hl` ` ` `in` ` ` `headlines` `]` `def` ` ` `aljazeera_articles` `(` `url` `,` ` ` `page_data` `):` `  ` `    ` `soup` ` ` `=` ` ` `BeautifulSoup` `(` `page_data` `,` ` ` `'lxml'` `)` `    ` `def` ` ` `match` `(` `tag` `):` `        ` `return` ` ` `(` `            ` `tag` `.` `text` ` ` `and` ` ` `tag` `.` `has_attr` `(` `'href'` `)` `            ` `and` ` ` `tag` `[` `'href'` `]` `.` `startswith` `(` `'/news'` `)` `            ` `and` ` ` `tag` `[` `'href'` `]` `.` `endswith` `(` `'.html'` `)` `        ` `)` `    ` `headlines` ` ` `=` ` ` `soup` `.` `find_all` `(` `match` `)` `    ` `return` ` ` `[(` `url` ` ` `+` ` ` `hl` `[` `'href'` `],` ` ` `hl` `.` ` ` `text` `,` ` ` `'aljazeera'` `)` `            ` `for` ` ` `hl` ` ` `in` ` ` `headlines` `]` `app` ` ` `=` ` ` `web` `.` `Application` `()` `app` `.` `router` `.` `add_get` `(` `'/news'` `,` ` ` `news` `)` `web` `.` `run_app` `(` `app` `,` ` ` `port` `=` `8080` `)` The  `news()`  function is the handler for the  */news*  URL on our server. It returns the HTML page showing all the headlines. Here, we have only two news websites to be scraped: CNN and Al Jazeera. More could easily be added, but then additional postprocessors would also have to be added, just like the  `cnn_articles()`  and  `aljazeera_articles()`  functions that are customized to extract headline data. For each news site, we create a task to fetch and process the HTML page data for its front page. Note that we unpack the tuple ( `(*s)` ) since the  `news_fetch()` coroutine function takes both the URL and the postprocessing function as parameters. Each  `news_fetch()`  call will return a  *list of tuples*  as headline results, in the form  `<article URL>` ,  `<article title>` . All the tasks are gathered together into a single  `Future`  ( `gather()`  returns a future representing the state of all the tasks being gathered), and then we immediately  `await`  the completion of that future. This line will suspend until the future completes. **96 ** **| ** **Chapter 4: 20 Asyncio Libraries You Aren’t Using (But…Oh, Never Mind)**