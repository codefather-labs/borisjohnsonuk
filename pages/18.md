1  Research in this area seems hard to find, but the numbers seem to be around 50 microseconds per threaded context switch on Linux on modern hardware. To give a (very) rough idea: one thousand threads implies 50 ms total cost just for the context switching. It does add up, but it isn’t going to wreck your application either. 2  The  *global interpreter lock*  (GIL) makes the Python interpreter code (not  *your*  code!) thread-safe by locking the processing of each opcode; it has the unfortunate side effect of effectively pinning the execution of the interpreter to a single CPU, and thus preventing multicore parallelism. 3  This is similar to why JavaScript lacks a GIL “problem”: there is only one thread. Network programming is  *not*  one of those domains. The key insight is that network programming involves a great deal of “waiting for things to happen,” and because of this, we don’t need the operating system to efficiently distribute our tasks over multiple CPUs. Furthermore, we don’t need the risks that preemptive multitasking brings, such as race conditions when working with shared memory. However, there is a great deal of misinformation about other supposed benefits of event-based programming models. Here are a few of the things that just ain’t so: *Asyncio will make my code blazing fast.* Unfortunately, no. In fact, most benchmarks seem to show that threading solutions are slightly faster than their comparable Asyncio solutions. If the extent of concurrency itself is considered a performance metric, Asyncio  *does*  make it a bit easier to create very large numbers of concurrent socket connections, though. Operating systems often have limits on how many threads can be created, and this number is significantly lower than the number of socket connections that can be made. The OS limits can be changed, but it is certainly easier to do with Asyncio. And while we expect that having many thousands of threads should incur extra  *context-switching*  costs that coroutines avoid, it turns out to be difficult to benchmark this in practice. 1  No, speed is not the benefit of Asyncio in Python; if that’s what you’re after, try  *Cython*  instead! *Asyncio makes threading redundant.* Definitely not! The true value of threading lies in being able to write multi-CPU programs, in which different computational tasks can share memory. The numerical library  `numpy` , for instance, already makes use of this by speeding up certain matrix calculations through the use of multiple CPUs, even though all the memory is shared. For sheer performance, there is no competitor to this programming model for CPU-bound computation. *Asyncio removes the problems with the GIL.* Again, no. It is true that Asyncio is not  *affected*  by the GIL, 2  but this is only because the GIL affects multithreaded programs. The “problems” with the GIL that people refer to occur because it prevents true multicore parallelism when using threads. Since Asyncio is single-threaded (almost by definition), it is unaffected by the GIL, but it cannot benefit from multiple CPU cores either. 3  It is also **What Problem Is Asyncio Trying to Solve? ** **| ** **7**